---
layout: default
title: Research
permalink: /research
---
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
 

##  <center> Neural network theory and algorithms

A neural network is a structurally simple parametric family with powerful representation capabilities in which an input propagates through a network  of  parallel matrix multiplications and activation units.
The aim is to find (learn, train) a neural network representation $$L$$ of a target function $$f$$ such that $$L(x) $$ approximates \\( f(x)\\), given noisy data $$y_i=f(x_i)+\epsilon_i$$, $$i=1,\dots, K$$.  $$f$$ can be  an image classifier, solution to a PDE, a specific parameter associated with a model, etc. Some challenges that arise are

* Neural network  architectures are typically selected by trial and error,  and often it is not clear beforehand how many layers and nodes should be taken in the network;

* Models  lack  interpretabilty, in particular it is not clear what the weights <spam>$$\{a_j,b_j,c_j\}_{j=1}^m$$ represent;</span>

* The training process is computationally expensive and there is not a good initialization method for the training;

* There are many stability issues, both during the training process (sensitivity to initialization) and post training (sensitive to adversarial attacks), and many others.

To better understand and address some of these questions, in this effort we investigate shallow neural networks, which are better suited for mathematical analysis. 
 
## <center> Joint sparse recovery

## <center> Dynamical sampling
		

 	<a href="https://mybinder.org/v2/gh/a-petr/gds/HEAD?filepath=gds.ipynb"		 >Binder link for GDS</a>

This is a mixed environment where you can have normal text and $c = \pm\sqrt{a^2 + b^2}$ fenced math. \$!


